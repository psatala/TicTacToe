{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TicTacToe.ipynb","provenance":[],"collapsed_sections":["4z1zhkAx-Ox9","6VMZOsgo-0lK","kuHI57RS-72E","gpntBs9Z-qqp","ofcJ0MJt_F-u","kuL6D5kV_K2L","6BXhjrZe_XpN","OaOwkDTI_u_c","uT-NEBVl_5Hw","TQ7Ue6ny__Dm","i2BgIhWTAMTp"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iIUAvOCz4vvz","colab_type":"text"},"source":["#Training\n","This notebook trains a model to play extended version of TicTacToe (aka gomoku).\n","The approach chosen for this problem is a reinforcement learning method known as policy gradient (with the help of discounted rewards)."]},{"cell_type":"markdown","metadata":{"id":"4z1zhkAx-Ox9","colab_type":"text"},"source":["###Setup\n","Setting up the notebook, necessary python modules and google drive - where the file with the model will be saved."]},{"cell_type":"code","metadata":{"id":"paC3vAVVf3ha","colab_type":"code","colab":{}},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPn61dbx-YBY","colab_type":"code","outputId":"3b7a8f03-0023-4f68-f1b5-9832d631451a","executionInfo":{"status":"ok","timestamp":1583600982995,"user_tz":-60,"elapsed":1741,"user":{"displayName":"Piotr Satała","photoUrl":"","userId":"14427926474650520640"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import random\n","import math\n","\n","print(torch.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w4McabW35Ibf","colab_type":"code","outputId":"1cd7318c-f29f-42da-9622-83e2152aacb8","executionInfo":{"status":"ok","timestamp":1583600985298,"user_tz":-60,"elapsed":4022,"user":{"displayName":"Piotr Satała","photoUrl":"","userId":"14427926474650520640"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["drive.mount(\"/content/gdrive\", force_remount = True)\n","path = Path(\"/content/gdrive/My Drive/Projects/TicTacToe\")\n","path.mkdir(parents=True, exist_ok=True)\n","modelPath = Path(str(path) + \"/model.pt\")\n","modelPath"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["PosixPath('/content/gdrive/My Drive/Projects/TicTacToe/model.pt')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"6VMZOsgo-0lK","colab_type":"text"},"source":["###Win detection\n","Function for detecting whether or not someone has won."]},{"cell_type":"code","metadata":{"id":"a8C8auds9mcI","colab_type":"code","colab":{}},"source":["def winDetection(board, x, y, inALine, value):\n","\n","    moveX = [1, 0, 1, 1]   #move in x direction\n","    moveY = [0, 1, 1, -1]  #move in y direction\n","\n","    for i in range(4):\n","        sum = 1\n","        for j in range(1, inALine):\n","            if(board[y + j * moveY[i]][x + j * moveX[i]] != value):\n","               break\n","            sum += 1\n","        for j in range(1, inALine):\n","            if(board[y - j * moveY[i]][x - j * moveX[i]] != value):\n","               break\n","            sum += 1\n","        if sum >= inALine:\n","            return True\n","    \n","    #no winner yet\n","    return False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kuHI57RS-72E","colab_type":"text"},"source":["###Parameters\n","Meta paramters for the game."]},{"cell_type":"code","metadata":{"id":"miyrBtgX-x9i","colab_type":"code","colab":{}},"source":["boardGameSize = 5\n","boardTotalSize = boardGameSize + 2\n","inALine = 4\n","discountRate = 0.9"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpntBs9Z-qqp","colab_type":"text"},"source":["###Transform board to input into nn\n","Data from board must be transformed to match model's input."]},{"cell_type":"code","metadata":{"id":"jeuZ8uXqDist","colab_type":"code","colab":{}},"source":["def transformInput(board, turn):\n","  boardGameSize = len(board) - 2\n","  boardExtended = np.repeat(board[1:boardGameSize + 1, 1:boardGameSize + 1].flatten()[:, np.newaxis], 2, axis=1) #create new np array from two copies of sliced board\n","  boardExtended = boardExtended.transpose()\n","  \n","  if turn != 1: \n","    boardExtended *= -1; #change perspective for red's move\n","  \n","  boardExtended[0][boardExtended[0] < 0] = 0 #filter first copy\n","  boardExtended[1][boardExtended[1] > 0] = 0 #filter second copy\n","\n","  boardExtended[1] *= -1\n","  \n","  boardExtended = boardExtended.flatten()\n","  #boardExtended = boardExtended.reshape(1, len(boardExtended))\n","  \n","  return boardExtended"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ofcJ0MJt_F-u","colab_type":"text"},"source":["###Define model\n","Class with the model definition. In current version - a simple fully connected neural network."]},{"cell_type":"code","metadata":{"id":"KZEacY-72kE0","colab_type":"code","colab":{}},"source":["class Model(torch.nn.Module):\n","  def __init__(self, sizeFirst, sizeHidden, sizeLast):\n","    super(Model, self).__init__()\n","    self.linear1 = torch.nn.Linear(sizeFirst, sizeHidden, bias = True)\n","    self.linear2 = torch.nn.Linear(sizeHidden, sizeLast, bias = True)\n","\n","  def forward(self, x):\n","    hidden = self.linear1(x)\n","    hidden = torch.nn.functional.relu(hidden)\n","\n","    y_pred = self.linear2(hidden)\n","    y_pred = torch.nn.functional.softmax(y_pred, dim=-1)\n","    \n","    return y_pred\n","\n","\n","model = Model((boardGameSize**2) * 2, 32, boardGameSize**2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kuL6D5kV_K2L","colab_type":"text"},"source":["###Get action from nn's output\n","Filtering results, normalising and additionaly sampling at random."]},{"cell_type":"code","metadata":{"id":"vrSvHtMTGhF7","colab_type":"code","colab":{}},"source":["def filterResults(res, board):\n","  res = res.reshape(boardGameSize**2)\n","  for i in range(boardGameSize):\n","    for j in range(boardGameSize):\n","      if board[i + 1][j + 1] != 0:\n","        res[i * boardGameSize + j] = 0\n","  return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"66qaa9I1KM4e","colab_type":"code","colab":{}},"source":["def normalize(res):\n","  temp = res.sum()\n","  for i in range(len(res)):\n","    res[i] /= temp\n","  return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SJoK5EhE3fyv","colab_type":"code","colab":{}},"source":["def sampleRandom(res, board):\n","  boardGameSize = len(board) - 2\n","  tempArray = np.random.permutation(boardGameSize**2)\n","  for i in range(len(tempArray)):\n","    if res[tempArray[i]] > 0:\n","      return tempArray[i]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BXhjrZe_XpN","colab_type":"text"},"source":["###Play game\n","Big function with the game loop."]},{"cell_type":"code","metadata":{"id":"SN3slZZJWVV7","colab_type":"code","colab":{}},"source":["#mode=1: selfplay\n","#mode=2: agent vs random opponent\n","def playGame(model, boardGameSize, inALine, discountRate = 0.9, mode = 1):\n","  \n","  #inits\n","  \n","  board = np.zeros([boardTotalSize, boardTotalSize], dtype = int)\n","  \n","  nMoves = 0\n","  isWinnerFound = False\n","  winner = 0\n","  turn = 1\n","  \n","  listOfBoardStates = []\n","  listOfDecisions = []\n","  \n","  random.seed()\n","  \n","  #game loop\n","  \n","  while not isWinnerFound:\n","  \n","    #check draw\n","    if nMoves == boardGameSize**2:\n","      break\n","    nMoves += 1\n","    \n","    #transform and save input\n","    nnInput = transformInput(board, turn)\n","    nnInput = torch.from_numpy(nnInput).float()\n","    listOfBoardStates.append(nnInput)\n","    \n","    #calculate probability distribution\n","    res = model(nnInput)\n","    res = filterResults(res, board)\n","    res = normalize(res)\n","    \n","    \n","    #sample move (agent)\n","    m = torch.distributions.Categorical(res)\n","    index = m.sample()\n","    \n","    #sample move (random opponent)\n","    if mode == 2 and turn == -1:\n","      index = sampleRandom(res, board)\n","    \n","    #make move\n","    a = (index // boardGameSize) + 1\n","    b = (index % boardGameSize) + 1\n","    if board[a][b] != 0:\n","      print(\"Noooo!\")\n","    board[a][b] = turn\n","    \n","    #save move\n","    listOfDecisions.append(index)\n","    \n","    #check if someone won\n","    if winDetection(board, b, a, inALine, turn):\n","      isWinnerFound = True\n","      winner = turn\n","    \n","    #change turn\n","    turn *= -1\n","  \n","  listOfLabels = torch.tensor(listOfDecisions)\n","  return listOfBoardStates, listOfLabels, winner"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OaOwkDTI_u_c","colab_type":"text"},"source":["###Calculate rewards and save data from game"]},{"cell_type":"code","metadata":{"id":"7gx44jUKcXug","colab_type":"code","colab":{}},"source":["def calculateRewards(listOfBoardStates, winner, discountRate = 0.9):\n","  rewards = torch.zeros([len(listOfBoardStates)], dtype=float)\n","  if winner != 0:\n","    rewards[-1] = 1\n","    rewards[-2] = -1\n","    for i in range(len(listOfBoardStates) - 3, -1, -1):\n","      rewards[i] = rewards[i + 2] * discountRate\n","  \n","    rewards -= torch.mean(rewards)\n","    rewards /= torch.std(rewards)\n","  \n","  return rewards"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uT-NEBVl_5Hw","colab_type":"text"},"source":["###Play against random opponent\n","Playing against an opponent whose decisions are completely random."]},{"cell_type":"code","metadata":{"id":"2xSUXg0Li1yD","colab_type":"code","colab":{}},"source":["def playAgainstRandom(nRounds):\n","  winCounter = 0\n","\n","  for i in range(nRounds):\n","    _, __, winner = playGame(model, boardGameSize, inALine, mode=2)\n","    if winner == 1:\n","      winCounter += 1\n","  \n","  return winCounter / float(nRounds)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQ7Ue6ny__Dm","colab_type":"text"},"source":["###Calculate time for a set of games\n","Benchmarking model's performance across the set of a 100 games against random opponent."]},{"cell_type":"code","metadata":{"id":"epCKOlx57UNz","colab_type":"code","outputId":"b30a6688-d696-4d91-8be4-ce8da3751f0e","executionInfo":{"status":"ok","timestamp":1583600987875,"user_tz":-60,"elapsed":6403,"user":{"displayName":"Piotr Satała","photoUrl":"","userId":"14427926474650520640"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import time\n","\n","startTime = time.time()\n","score = playAgainstRandom(100)\n","endTime = time.time()\n","\n","score, endTime - startTime"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.58, 2.4014534950256348)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"i2BgIhWTAMTp","colab_type":"text"},"source":["###Train"]},{"cell_type":"code","metadata":{"id":"ypvWMBHym5kr","colab_type":"code","colab":{}},"source":["#parameters for training\n","baseLR = 1e-3\n","epoch = 0\n","nEpochs = (10 ** 4)\n","validateEvery = 10 ** 3\n","nRoundsOfValidation = 10 ** 2\n","decayEvery = 3 * (10 ** 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHIhzfj8036P","colab_type":"code","colab":{}},"source":["#optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr = baseLR)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVzh4-S_KzNT","colab_type":"code","colab":{}},"source":["#training across one batch - one full game\n","def train(model, optimizer, data):\n","  for input, lb, rew in data:\n","    optimizer.zero_grad()\n","    result = model(input)\n","    m = torch.distributions.Categorical(result)\n","    loss = -m.log_prob(lb) * rew\n","    \n","    if loss == loss:\n","      loss.backward()\n","      optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5uv5lGHgAbu","colab_type":"code","colab":{}},"source":["#dynamic learning rate\n","def adjustLR(optimizer, epoch, baseLR, decayEvery):\n","  for g in optimizer.param_groups:\n","    g['lr'] = baseLR * (0.1 ** (epoch // decayEvery))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgA2jKy_-ARM","colab_type":"code","outputId":"93ff7874-ad26-4f39-963a-947e7973ac73","executionInfo":{"status":"ok","timestamp":1583601232324,"user_tz":-60,"elapsed":250805,"user":{"displayName":"Piotr Satała","photoUrl":"","userId":"14427926474650520640"}},"colab":{"base_uri":"https://localhost:8080/","height":353}},"source":["#training\n","for i in range(nEpochs):\n","  listOfBoardStates, listOfDecisions, winner = playGame(model, boardGameSize, inALine, discountRate)\n","  rewards = calculateRewards(listOfBoardStates, winner, discountRate)\n","  zipper = zip(listOfBoardStates, listOfDecisions, rewards)\n","  data = list(zipper)\n","  \n","  train(model, optimizer, data)\n","  epoch += 1\n","  \n","  adjustLR(optimizer, epoch, baseLR, decayEvery)\n","\n","  if epoch % validateEvery == 0:\n","    score = playAgainstRandom(nRoundsOfValidation)\n","    print(epoch, score)\n","    for param in optimizer.param_groups:\n","      print(\"%.7f\" % param['lr'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1000 0.79\n","0.0010000\n","2000 0.75\n","0.0010000\n","3000 0.79\n","0.0001000\n","4000 0.79\n","0.0001000\n","5000 0.84\n","0.0001000\n","6000 0.84\n","0.0000100\n","7000 0.87\n","0.0000100\n","8000 0.86\n","0.0000100\n","9000 0.86\n","0.0000010\n","10000 0.86\n","0.0000010\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jeFWp0uQ7sz3","colab_type":"code","colab":{}},"source":["#save model\n","torch.save(model.state_dict(), modelPath)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VfjEsQ3Z9Y5","colab_type":"code","colab":{}},"source":["#print parameter values\n","for name, param in model.named_parameters():\n","  if param.requires_grad:\n","    print(name, param.data)"],"execution_count":0,"outputs":[]}]}